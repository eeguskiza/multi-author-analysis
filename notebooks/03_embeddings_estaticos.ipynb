{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a73950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear embeddings estáticos desde processed.\n",
    "# Modelo: Word2Vec entrenado en TRAIN.\n",
    "# Salidas: modelo, vocab, sentence-embeddings por split y cobertura OOV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ac26b",
   "metadata": {},
   "source": [
    "#### ***Imports y config***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52df0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "SEED = 42\n",
    "DIM = 128\n",
    "EPOCHS = 8\n",
    "MIN_COUNT = 5\n",
    "WINDOW = 5\n",
    "SG = 1           # 1 skip-gram, 0 CBOW\n",
    "\n",
    "np.random.seed(SEED)\n",
    "pd.set_option(\"display.max_colwidth\", 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61c02a",
   "metadata": {},
   "source": [
    "#### ***Rutas***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea89954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root():\n",
    "    p = Path.cwd()\n",
    "    for cand in [p, *p.parents]:\n",
    "        if (cand / \"data\" / \"processed\").exists():\n",
    "            return cand\n",
    "    raise FileNotFoundError(\"No encuentro data/processed.\")\n",
    "\n",
    "ROOT = find_root()\n",
    "PROC = ROOT / \"data\" / \"processed\"\n",
    "FEAT = ROOT / \"features\" / \"embeddings_static\"\n",
    "FEAT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NIVELES = [\"easy\",\"medium\",\"hard\"]\n",
    "SPLITS = [\"train\",\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ef374",
   "metadata": {},
   "source": [
    "#### ***Carga de processed → tokens***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ec5572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171602, 5) (36558, 5)\n"
     ]
    }
   ],
   "source": [
    "def cargar_split(split):\n",
    "    rows = []\n",
    "    for level in NIVELES:\n",
    "        p = PROC / level / split / \"sentences.jsonl\"\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        df = pd.read_json(p, lines=True, dtype={\"sent_id\": int})\n",
    "        df[\"level\"] = level\n",
    "        df[\"split\"] = split\n",
    "        rows.append(df[[\"level\",\"split\",\"doc_id\",\"sent_id\",\"text_norm\"]])\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"level\",\"split\",\"doc_id\",\"sent_id\",\"text_norm\"])\n",
    "    out = pd.concat(rows, ignore_index=True).sort_values([\"level\",\"doc_id\",\"sent_id\"])\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "df_train = cargar_split(\"train\")\n",
    "df_val   = cargar_split(\"validation\")\n",
    "print(df_train.shape, df_val.shape)\n",
    "\n",
    "def texto_a_tokens(s):\n",
    "    # text_norm ya está en minúsculas y normalizado\n",
    "    return str(s).split()\n",
    "\n",
    "sentences_train = [texto_a_tokens(t) for t in df_train[\"text_norm\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34baa758",
   "metadata": {},
   "source": [
    "#### ***Entrenamiento Word2Vec en train***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16705202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 17403\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec(\n",
    "    sentences=sentences_train,\n",
    "    vector_size=DIM,\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    sg=SG,\n",
    "    negative=10,\n",
    "    epochs=EPOCHS,\n",
    "    workers=4,\n",
    "    seed=SEED\n",
    ")\n",
    "kv: KeyedVectors = w2v.wv\n",
    "print(\"Vocab size:\", len(kv.key_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e46fb8a",
   "metadata": {},
   "source": [
    "#### ***Guardado del modelo y vocab***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac937dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo KeyedVectors para aligerar\n",
    "kv.save(str(FEAT / \"w2v.kv\"))\n",
    "\n",
    "# Vocabulario con frecuencias\n",
    "vocab_rows = []\n",
    "for w, idx in kv.key_to_index.items():\n",
    "    cnt = kv.get_vecattr(w, \"count\") if kv.has_index_for(w) else None\n",
    "    vocab_rows.append((w, idx, cnt))\n",
    "pd.DataFrame(vocab_rows, columns=[\"token\",\"index\",\"count\"]).to_csv(FEAT / \"w2v_vocab.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2eb89",
   "metadata": {},
   "source": [
    "#### ***Embeddings de frase train y validation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "951e691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "train: (171602, 128) cobertura oraciones con al menos 1 token conocido: 0.987\n",
      "val  : (36558, 128) cobertura oraciones con al menos 1 token conocido: 0.988\n"
     ]
    }
   ],
   "source": [
    "def oracion_a_vector(tokens, kv, dim=DIM):\n",
    "    vecs = [kv[t] for t in tokens if t in kv]\n",
    "    if not vecs:\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "    v = np.mean(vecs, axis=0)\n",
    "    return v.astype(np.float32)\n",
    "\n",
    "def construir_sentence_embeddings(df, kv, split):\n",
    "    X = np.zeros((len(df), kv.vector_size), dtype=np.float32)\n",
    "    seen = 0\n",
    "    for i, toks in enumerate(map(texto_a_tokens, df[\"text_norm\"])):\n",
    "        vec = oracion_a_vector(toks, kv)\n",
    "        if vec.any():\n",
    "            seen += 1\n",
    "        X[i] = vec\n",
    "    idx = df[[\"level\",\"split\",\"doc_id\",\"sent_id\"]].reset_index(drop=True)\n",
    "    np.save(FEAT / f\"S_{split}.npy\", X)\n",
    "    idx.to_csv(FEAT / f\"S_{split}_index.csv\", index=False)\n",
    "    return X, idx, seen\n",
    "\n",
    "Xtr, idx_tr, seen_tr = construir_sentence_embeddings(df_train, kv, \"train\")\n",
    "Xva, idx_va, seen_va = construir_sentence_embeddings(df_val,   kv, \"validation\")\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(\"train:\", Xtr.shape, \"cobertura oraciones con al menos 1 token conocido:\", round(seen_tr/len(df_train), 3))\n",
    "print(\"val  :\", Xva.shape, \"cobertura oraciones con al menos 1 token conocido:\", round(seen_va/len(df_val), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7a1ab",
   "metadata": {},
   "source": [
    "#### ***Cobertura OOV y resumen***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e79c8609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'tokens_total': 3112716,\n",
       "  'tokens_conocidos': 3064989,\n",
       "  'coverage': 0.9847},\n",
       " 'validation': {'tokens_total': 668938,\n",
       "  'tokens_conocidos': 655930,\n",
       "  'coverage': 0.9806},\n",
       " 'vocab_size': 17403,\n",
       " 'dim': 128,\n",
       " 'min_count': 5,\n",
       " 'window': 5,\n",
       " 'epochs': 8,\n",
       " 'sg': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cobertura_tokens(df, kv):\n",
    "    total = 0\n",
    "    hit = 0\n",
    "    for toks in map(texto_a_tokens, df[\"text_norm\"]):\n",
    "        total += len(toks)\n",
    "        hit += sum(1 for t in toks if t in kv)\n",
    "    rate = hit / total if total else 0.0\n",
    "    return {\"tokens_total\": int(total), \"tokens_conocidos\": int(hit), \"coverage\": float(round(rate, 4))}\n",
    "\n",
    "cov = {\n",
    "    \"train\": cobertura_tokens(df_train, kv),\n",
    "    \"validation\": cobertura_tokens(df_val, kv),\n",
    "    \"vocab_size\": len(kv.key_to_index),\n",
    "    \"dim\": kv.vector_size,\n",
    "    \"min_count\": MIN_COUNT,\n",
    "    \"window\": WINDOW,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"sg\": SG\n",
    "}\n",
    "(Path(FEAT) / \"w2v_resumen.json\").write_text(json.dumps(cov, indent=2), encoding=\"utf-8\")\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78b668",
   "metadata": {},
   "source": [
    "#### ***Exportables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2cd26da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado:\n",
      "- /Users/eeguskiza/Documents/Deusto/2025/NLP/multi-author-analysis/features/embeddings_static/w2v.kv\n",
      "- /Users/eeguskiza/Documents/Deusto/2025/NLP/multi-author-analysis/features/embeddings_static/w2v_vocab.csv\n",
      "- /Users/eeguskiza/Documents/Deusto/2025/NLP/multi-author-analysis/features/embeddings_static/S_train.npy y /Users/eeguskiza/Documents/Deusto/2025/NLP/multi-author-analysis/features/embeddings_static/S_train_index.csv\n",
      "- /Users/eeguskiza/Documents/Deusto/2025/NLP/multi-author-analysis/features/embeddings_static/S_validation.npy y /Users/eeguskiza/Documents/Deusto/2025/NLP/multi-author-analysis/features/embeddings_static/S_validation_index.csv\n",
      "- /Users/eeguskiza/Documents/Deusto/2025/NLP/multi-author-analysis/features/embeddings_static/w2v_resumen.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardado:\")\n",
    "print(\"-\", FEAT / \"w2v.kv\")\n",
    "print(\"-\", FEAT / \"w2v_vocab.csv\")\n",
    "print(\"-\", FEAT / \"S_train.npy\", \"y\", FEAT / \"S_train_index.csv\")\n",
    "print(\"-\", FEAT / \"S_validation.npy\", \"y\", FEAT / \"S_validation_index.csv\")\n",
    "print(\"-\", FEAT / \"w2v_resumen.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4647f6",
   "metadata": {},
   "source": [
    "# Informe breve — `03_embeddings_estaticos.ipynb`\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Entrenar Word2Vec en **train** y derivar embeddings de frase por media de vectores de palabra. \n",
    "\n",
    "## Configuración\n",
    "\n",
    "* `vector_size=128`, `window=5`, `min_count=5`, `sg=1`, `epochs=8`, `negative=10`, `seed=42`. \n",
    "\n",
    "## Resultados\n",
    "\n",
    "* **Vocabulario**: 17 403 tokens. \n",
    "* **Sentence embeddings**:\n",
    "\n",
    "  * train `(171602, 128)` con cobertura de oraciones ≥1 token conocido **0.987**. \n",
    "  * validation `(36558, 128)` con cobertura **0.988**. \n",
    "* **Cobertura a nivel de tokens**:\n",
    "\n",
    "  * train **0.9847**\n",
    "  * validation **0.9806**. \n",
    "\n",
    "## Salidas\n",
    "\n",
    "* Modelo: `features/embeddings_static/w2v.kv`\n",
    "* Vocabulario: `w2v_vocab.csv`\n",
    "* Embeddings de frase: `S_train.npy`, `S_validation.npy`\n",
    "* Índices: `S_{split}_index.csv`\n",
    "* Resumen: `w2v_resumen.json`. \n",
    "\n",
    "## Uso previsto\n",
    "\n",
    "* Baselines rápidos de cambio de autor con similitud entre frases/ventanas.\n",
    "* Insumo para modelos clásicos o híbridos en E3/E4. \n",
    "\n",
    "## Notas\n",
    "\n",
    "* Ajuste solo en **train**.\n",
    "* Cobertura alta y dimensiones compactas. Si hiciera falta más recall léxico, bajar `min_count` a 3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
